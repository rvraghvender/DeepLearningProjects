{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5vK2hG8ahCp1UugdFYfTQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rvraghvender/DeepLearningProjects/blob/main/NaturalLanguageProcessing/CharacterLevelLanguageModel/Character_level_language_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Character level language model - Dinosaurus Island\n",
        "\n",
        "Welcome to Dinosaurus Island! 65 millions years ago, dinosaurs existed and they have returned in this project.\n",
        "\n",
        "We are in charge of a special task: Leading biology researchers are creating new breeds of dinosuars and bringing them to life on earth, and our job is to give names to these dinosaurs. If a dinosaurs does not like its name, it might go berserk;). So, choose wisely!\n",
        "\n",
        "<table>\n",
        "<td>\n",
        "<img src=\"https://github.com/rvraghvender/DeepLearningProjects/blob/main/NaturalLanguageProcessing/CharacterLevelLanguageModel/images/dino.jpg?raw=true\" style=\"width:250;height:300px;\">\n",
        "\n",
        "</td>\n",
        "\n",
        "</table>\n",
        "\n",
        "Luckily, we'are equipped with some deep learning concepts, and we will use it to save the day! Our assistant has collected a list of all dinosuars names they could find, and compiled them into this [dataset](dinos.txt).  To create new dinosaur names, we will build a character-level language model to generate new names. Our algorithm, will learn the different name patterns, and randomly generate new names.\n",
        "\n",
        "For completetion of this project, we need to:\n",
        " * Store text data for processing using an RNN\n",
        " * Build a character-level text generation model using an RNN\n",
        " * Sample novel sequences in an RNN\n",
        " * Explain the vanishing/exploding gradient problem in RNNs\n",
        " * Apply gradient clipping as a solution for exploding gradients\n",
        "\n"
      ],
      "metadata": {
        "id": "i1F__G-HIyYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Table of Contents\n",
        "\n",
        "- [Packages](#0)\n",
        "- [1 - Problem Statement](#1)\n",
        "    - [1.1 - Dataset and Preprocessing](#1-1)\n",
        "    - [1.2 - Overview of the Model](#1-2)\n",
        "- [2 - Building Blocks of the Model](#2)\n",
        "    - [2.1 - Clipping the Gradients in the Optimization Loop](#2-1)\n",
        "        - [TODO 1 - clip](#ex-1)\n",
        "    - [2.2 - Sampling](#2-2)\n",
        "        - [TODO 2 - sample](#ex-2)\n",
        "- [3 - Building the Language Model](#3)\n",
        "    - [3.1 - Gradient Descent](#3-1)\n",
        "        - [TODO 3 - optimize](#ex-3)\n",
        "    - [3.2 - Training the Model](#3-2)\n",
        "        - [TODO 4 - model](#ex-4)\n",
        "- [4 - Writing like Shakespeare](#4)\n",
        "- [5 - References](#5)"
      ],
      "metadata": {
        "id": "tjtysuTfLCHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Packages"
      ],
      "metadata": {
        "id": "xZMd22iyLQu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from utils import *\n",
        "import random\n",
        "import pprint\n",
        "import copy"
      ],
      "metadata": {
        "id": "LyJxcxExK-YG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 - Problem Statment\n",
        "\n",
        "### 1.1 - Dataset and Preprocessing\n",
        "\n",
        "Run the following cell to read the dataset of dinosaurs names, create a list of unique characters (such as a-z), and compute the dataset and vocabulary size.'"
      ],
      "metadata": {
        "id": "qMRgRHEiLfhc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8iBYRSboIqBA",
        "outputId": "792e77de-3f3f-43a3-c12c-51fb5c72928b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 19909 total characters and 27 unique characters in our data.\n"
          ]
        }
      ],
      "source": [
        "data = open('dinos.txt', 'r').read()\n",
        "data = data.lower()\n",
        "chars = list(set(data))\n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print(f'There are {data_size} total characters and {vocab_size} unique characters in our data.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The characters are a-z (26 characters) plus \"\\n\" (or newline character)\n",
        "- Here the newline character \"\\n\" plays a role similar to the <EOS> (or \"End of sentence\") token.\n",
        "    - Here, \"\\n\" indicates the end of the dinosaur name rather than the end of sentence.\n",
        "- `char_to_ix`: In the cell below, we'll create a Python dictionary to map each character to an index from 0-26.\n",
        "- `ix_to_char`: Then, we will create second Python dictionary that maps each index back to the correspoding character.\n",
        "    - This will help us to figure out which index corresponds to which character in the probability distribution output of the softmax layer."
      ],
      "metadata": {
        "id": "j-5xx2TDM6iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(chars)\n",
        "print(chars)"
      ],
      "metadata": {
        "id": "QiF0D4Z8MLS2",
        "outputId": "47be9b22-abf7-465f-d431-9eb1e1de8917",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
        "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "pp.pprint(ix_to_char)"
      ],
      "metadata": {
        "id": "4hNfLpggOCVz",
        "outputId": "6c92c6f7-01e6-4487-bf3e-61fdd0ba235d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{   0: '\\n',\n",
            "    1: 'a',\n",
            "    2: 'b',\n",
            "    3: 'c',\n",
            "    4: 'd',\n",
            "    5: 'e',\n",
            "    6: 'f',\n",
            "    7: 'g',\n",
            "    8: 'h',\n",
            "    9: 'i',\n",
            "    10: 'j',\n",
            "    11: 'k',\n",
            "    12: 'l',\n",
            "    13: 'm',\n",
            "    14: 'n',\n",
            "    15: 'o',\n",
            "    16: 'p',\n",
            "    17: 'q',\n",
            "    18: 'r',\n",
            "    19: 's',\n",
            "    20: 't',\n",
            "    21: 'u',\n",
            "    22: 'v',\n",
            "    23: 'w',\n",
            "    24: 'x',\n",
            "    25: 'y',\n",
            "    26: 'z'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 - Overview of the Model\n",
        "\n",
        "Our model will have the following structure:\n",
        "\n",
        "- Initialize parameters\n",
        "- Run the optimization loop\n",
        "    - Forward propagation to compute the loss function\n",
        "    - Backward propagation to compute the gradients with respect to the loss function\n",
        "    - Clip the gradients to avoid exploding gradients\n",
        "    - Using the gradients, update our parameters with the gradient descent update rule.\n",
        "- Return the learned parameteres.\n",
        "\n",
        "<img src=\"https://github.com/rvraghvender/DeepLearningProjects/blob/main/NaturalLanguageProcessing/CharacterLevelLanguageModel/images/rnn.png?raw=true\" style=\"width:450;height:300px;\">\n",
        "<caption><center><font color='purple'><b>Figure 1</b>: Recurrent Neural Network, similar to \"Building a Recurrent Neural Network - Step by Step.\" notebook  </center></caption>\n",
        "\n",
        "- At each time-step, the RNN tries to predict what the next characters is, given the previous characters.\n",
        "- $X$ = ($x^{<1>}$, $x^{<2>}$, ... ,$x^{<T_x>}$) is a list of characters from the trainig set.\n",
        "- $Y$ = ($y^{<1>}$, $y^{<2>}$, ... ,$y^{<T_x>}$) is the same list of characters but shifted one character forward.\n",
        "- At every time-step $t$, $y^{<t>}$ = $x^{<t+1>}$. The prediction at time $t$ is the same as the input at the $t+1$.\n"
      ],
      "metadata": {
        "id": "aQ6VBww9O22q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Building Block of the Model\n",
        "\n",
        "In this part, we will build two important blocks of the overall model:\n",
        "\n",
        "    1. Gradient clipping: to avoid exploding the gradients\n",
        "    2. Sampling: a technique used to generate characters\n",
        "\n",
        "We will apply these two functions to build the model."
      ],
      "metadata": {
        "id": "ngm6-_EcQtI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 - Clipping the Gradients in the Optimization Loop\n",
        "\n",
        "In this section, we will implement `clip` function that we will call inside of our optimization loop.\n",
        "\n",
        "#### Exploding gradients\n",
        "\n",
        "- When gradients are very large, they're called \"exploding gradients\"\n",
        "- Exploding gradients make the training process more difficult, because the updates may be so large that they \"overshoot\" the optimal values during back propagation."
      ],
      "metadata": {
        "id": "lo4Ze-YdRH5D"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t7C_tLerRHJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jzBNgSlxOj3S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}